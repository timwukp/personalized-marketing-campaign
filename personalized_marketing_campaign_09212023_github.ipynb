{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3f88dd-0f5e-427e-84ee-8934982300d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Personalized Marketing Campaign Content Creation\n",
    "## Use case\n",
    "\n",
    "Assuming you are a marketing campaign manager, you're going to promote the flight ticket for Airline. At first, AI system will help you find out the target users segment; and then generate marketing promotion template for thoes target users. \n",
    "There has two AI engines\n",
    "1. Recommendation engine - Amazon Personalize service\n",
    "2. Content Generative engine - Amazon Bedrock service\n",
    "\n",
    "\n",
    "The pipeline - \n",
    "\n",
    "Marketing request-->Personalize-->retrive medata-->combine with PromptTemplate--> Langchain-->Amazon Bedrock & LLM--> Generate content -->save in JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920ca4a-a71d-4630-a6e4-577d95192ad1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we show how to generate personalized marketing campaign promotion for email, by contextual metadata and question template. \n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. The key aspects of this framework allow us to augment the Large Language Models by chaining together various components to create advanced use cases.\n",
    "\n",
    "In this notebook we will use the Bedrock API provided by LangChain. The prompt used in this example creates a custom LangChain prompt template for adding context to the text generation request. \n",
    "\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. The key aspects of this framework allow us to augment the Large Models and enable us to perform tasks which meet desired goals and unlock various use-cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11828a-243d-4808-9c92-e8caf4cebd37",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "Before we get started with the implementation we have to make sure that the required boto3 and botocore packages are installed. These will be used to leverage the Amazon Bedrock API client.\n",
    "\n",
    "Additionally we would need langchain one of  the latest versions 0.0.190 which has Amazon Bedrock class implemented under llms module. Also we are installing the transformers framework from HuggingFace, which we will use to quickly count the number of tokens in the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b7248-f29d-4abe-9aea-57471e676bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2c0a9-4838-4f2b-bb36-61c0cbcd62af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install boto3-1.26.162-py3-none-any.whl\n",
    "!python3 -m pip install botocore-1.29.162-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2be60b-480a-4524-8a1d-3529ebcb812d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain==0.0.190 --quiet\n",
    "%pip install transformers==4.24.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31093a39-704f-4a52-a4cb-d094cf930b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install ipywidgets\n",
    "\n",
    "# install timer counter \n",
    "%pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcee7d7-774f-4ade-b00e-e66052a9a8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "boto3_bedrock  = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daa1a8-d21a-410c-adbf-b253c2dabf80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Content generative engine, invoke the Bedrock LLM Model\n",
    "\n",
    "for more details for the parameters please refer to the bedrock api page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f5c74-30b1-4ae8-b044-282f5232653f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(\n",
    " service_name='bedrock',\n",
    " region_name='us-east-1',\n",
    " endpoint_url='https://bedrock.us-east-1.amazonaws.com'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ab211-dc91-444e-9968-3821d1e32326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock.list_foundation_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa1250-56cd-4b6d-b3d8-c62baac143ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# textgen_llm = Bedrock(model_id=\"amazon.titan-tg1-large\", client=boto3_bedrock)\n",
    "\n",
    "inference_modifier = {'max_tokens_to_sample':4096, \n",
    "                      \"temperature\":0.7,\n",
    "                      \"top_k\":250,\n",
    "                      \"top_p\":1,\n",
    "                      \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                     }\n",
    "\n",
    "textgen_llm = Bedrock(model_id = \"anthropic.claude-v2\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f310c88-bb7b-4dab-8980-ce29f5ea34fd",
   "metadata": {},
   "source": [
    "## Prepare contextual item medata, and question template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705c52c-e26d-4df7-af1d-e4ef22ec582f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_json_data(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "def on_confirm_button_click(b):\n",
    "    selected_metadata_file = metadata_dropdown.value\n",
    "    selected_template_file = template_dropdown.value\n",
    "\n",
    "    metadata = read_json_data(selected_metadata_file)\n",
    "    print(\"Metadata JSON data:\")\n",
    "    print(json.dumps(metadata, indent=4))\n",
    "\n",
    "    template = read_json_data(selected_template_file).get(\"Question\", \"\")\n",
    "    print(\"Ticketing Template:\")\n",
    "    print(template)\n",
    "\n",
    "metadata_dropdown = widgets.Dropdown(\n",
    "    options=[\"test-metadata.json\", \"other-metadata.json\"],  # Add more options if needed\n",
    "    description=\"Metadata File:\"\n",
    ")\n",
    "\n",
    "template_dropdown = widgets.Dropdown(\n",
    "    options=[\"ticketing-template.json\", \"other-template.json\"],  # Add more options if needed\n",
    "    description=\"Template File:\"\n",
    ")\n",
    "\n",
    "confirm_button = widgets.Button(description=\"Confirm button\")\n",
    "confirm_button.on_click(on_confirm_button_click)\n",
    "\n",
    "display(metadata_dropdown, template_dropdown, confirm_button)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c875074-4113-499d-9636-2d9c18b5af96",
   "metadata": {},
   "source": [
    "## Prepare the prompting template by metadata and question template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526db6f-112d-4b37-8fcd-62879c58b022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_output():\n",
    "    with open(\"test-metadata.json\", \"r\") as json_file:\n",
    "        metadata = json.load(json_file)\n",
    "\n",
    "    with open(\"ticketing-template.json\", \"r\") as json_file:\n",
    "        ticketing_template = json.load(json_file)\n",
    "\n",
    "    input_variables = list(metadata.keys())\n",
    "    template = ticketing_template.get(\"Question\", \"\")\n",
    "\n",
    "    multi_var_prompt = PromptTemplate(\n",
    "        input_variables=input_variables,\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    prompt = multi_var_prompt.format(**metadata)\n",
    "\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        print(prompt)\n",
    "    \n",
    "    \n",
    "    return prompt    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99e76d-46c4-457e-bee3-719c709a334b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "# Create the button widget\n",
    "button = widgets.Button(description=\"Generate Question\")\n",
    "\n",
    "# Define the event handler for the button\n",
    "button.on_click(update_output)\n",
    "\n",
    "# Display the button and output widget\n",
    "display(button, output)\n",
    "\n",
    "prompt = update_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02668000-4fce-4247-8210-743cb08b9ded",
   "metadata": {},
   "source": [
    "### Calculate the number of input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45bdfd5-ce76-42e9-81cd-b0892337d163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = textgen_llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf31e9-56c0-408f-a652-9e23de446aef",
   "metadata": {},
   "source": [
    "## Generate the content by the request of prompting question\n",
    "\n",
    "invoke using the prompt tempalate and expect to see a curated response back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f2d1e-1d11-489b-9097-f2449d01f70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "# Measure the time taken for text generation\n",
    "start_time = time.time()\n",
    "email = response[response.index('\\n')+1:]\n",
    "end_time = time.time()\n",
    "responding_time = end_time - start_time\n",
    "\n",
    "# Print the responding time\n",
    "print(\"Responding time:\", responding_time, \"seconds\")\n",
    "\n",
    "print(\"\\nEmail generation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caccf4d-6dc5-40eb-9732-a95767f076ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460879b2-86f9-48f1-87ce-6d5348a394e0",
   "metadata": {},
   "source": [
    "## Optional: Prompt Chaining\n",
    "\n",
    "Prompt chaining can allow you to accomplish a complex task by passing Claude multiple smaller and simpler prompts instead of a very long and detailed one. It can sometimes work better than putting all of a task's subtasks in a single prompt.\n",
    "\n",
    "Turning a long and complex prompt into a prompt chain can have a few advantages:\n",
    "\n",
    "You can write less complicated instructions.\n",
    "You can isolate parts of a problem that Claude is having trouble with to focus your troubleshooting efforts.\n",
    "You can check Claude's output in stages, instead of just at the end.\n",
    "\n",
    "https://docs.anthropic.com/claude/docs/prompt-chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43854fc7-7f4e-4a5d-b1b6-af2e18b726bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('top50inHK.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Define a prompt and additional statement\n",
    "\n",
    "additional_statement = \"These are the top 50 most fun and delicious places to recommend in Hong Kong, please enrich the itinerary, and generate again\"\n",
    "\n",
    "# Initialize empty lists for the title and body of each paragraph\n",
    "title_paragraphs = []\n",
    "body_paragraphs = []\n",
    "\n",
    "# Loop through each category and its items\n",
    "for category, items in data.items():\n",
    "    # Combine the category and items into a single string for the title\n",
    "    title = f\"{category}: {', '.join(items)}\"\n",
    "    # Create a body paragraph with a brief description or additional information\n",
    "    #body = \"Explore these fantastic places to make the most of your trip to Hong Kong.\"\n",
    "\n",
    "    # Append the title and body paragraphs to their respective lists\n",
    "    title_paragraphs.append(title)\n",
    "    #body_paragraphs.append(body)\n",
    "\n",
    "# Combine paragraphs with prompt and additional statement\n",
    "combined_paragraphs = \"\\n\\n\".join(title_paragraphs + [additional_statement] + body_paragraphs)\n",
    "promptChaining = f\"{prompt}\\n\\n{combined_paragraphs}\"\n",
    "\n",
    "# Print the final prompt\n",
    "print(promptChaining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833e547-f93a-494b-add2-a9c7e14347f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "response = textgen_llm(promptChaining)\n",
    "\n",
    "# Measure the time taken for text generation\n",
    "start_time = time.time()\n",
    "email2 = response[response.index('\\n')+1:]\n",
    "end_time = time.time()\n",
    "responding_time = end_time - start_time\n",
    "\n",
    "# Print the responding time\n",
    "print(\"Responding time:\", responding_time, \"seconds\")\n",
    "\n",
    "print(\"\\nEmail generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1064c57-27a4-48c5-911b-e4f1dfeff122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " print(email2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e2731d-3679-4a9a-aeef-d8e9d09ae087",
   "metadata": {},
   "source": [
    "## Save the content in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f594e-aff7-465e-b7b9-70002e2ad4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming you already have the 'email' variable with the email content\n",
    "# If not, you can replace this with the appropriate content.\n",
    "\n",
    "email_content = email.strip()\n",
    "\n",
    "# Create a dictionary with the email content\n",
    "email_dict = {\"email\": email_content}\n",
    "\n",
    "# Define the filename for the JSON file\n",
    "json_filename = \"email_content_english.json\"\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open(json_filename, \"w\") as json_file:\n",
    "    json.dump(email_dict, json_file, indent=4)\n",
    "\n",
    "print(f\"The email content has been saved as {json_filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82520d5-68e5-4a5d-b4f8-5939a566287b",
   "metadata": {},
   "source": [
    "## Text to Image (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536c72e-8dfa-4872-bcde-506ab7a9483f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --quiet \"pillow>=9.5,<10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a988d2-20b9-48ba-a1b0-ea8dfcfd528c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "from PIL import Image\n",
    "bedrock_client = boto3.client('bedrock' , 'us-east-1', endpoint_url='https://bedrock.us-east-1.amazonaws.com')\n",
    "bedrock_client.list_foundation_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab780d8-19aa-450c-ae0d-c83a33a3c544",
   "metadata": {},
   "source": [
    "## Prepare prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66815729-aab8-4675-a970-35c906f6aa91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dca501-3c1d-4540-b953-3df7a8620bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "negative_prompts = [\n",
    "    \"poorly rendered\",\n",
    "    \"poor background details\",\n",
    "    \"Not realistic enough\",\n",
    "    \"poorly drawn people\",\n",
    "    \"poorly drawn human eyes\",\n",
    "    \"poorly drawn human nose\",\n",
    "    \"poorly drawn human hair\",\n",
    "    \"poorly drawn human finger\",\n",
    "    \"no landmark\",\n",
    "    \"missing human fingers\",\n",
    "    \"poorly drawn aircraft\",\n",
    "    \"poorly quality of font\",\n",
    "    \"pixels too lower\",\n",
    "    \"city looks not prosperous enough\"\n",
    "    \"disfigured people features\",\n",
    "]\n",
    "style_preset = \"photographic\"  # (e.g. photographic, digital-art, cinematic, ...)\n",
    "#prompt = \"photo taken from above of an italian landscape. cloud is clear with few clouds. Green hills and few villages, a lake\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cdf8d-c7bb-4808-947b-95473ab5a4eb",
   "metadata": {},
   "source": [
    "### The Stability.ai Diffusion models support the following controls. \n",
    "\n",
    "cfg_scale: Prompt strength– Determines how much the final image portrays the prompt random generations. The range is 0—30, and the default value is 10.\n",
    "the \"cfg_scale\" essentially governs how much the image looks closer to the prompt or input image. The higher the CFG scale, the more the image will match your prompt. Conversely, a lower CFG scale value produces a better-quality image that may differ from the original prompt or image\n",
    "\n",
    "In Stable Diffusion, CFG stands for Classifier Free Guidance scale. CFG is the setting that controls how closely Stable Diffusion should follow your text prompt. It is applied in text-to-image (txt2img) and image-to-image (img2img) generations.\n",
    "\n",
    "The higher the CFG value, the more strictly it will follow your prompt, in theory. The default value is 10, which gives a good balance between creative freedom and following your direction. A value of 1 will give Stable Diffusion almost complete freedom, whereas values above 15 are quite restrictive.\n",
    "\n",
    "step: Generation step determines how many times the image is sampled. More steps can result in a more accurate result. The range is 0—150, and the default value is 5.\n",
    "\n",
    "Seed: The seed determines the initial noise setting. If you use the same seed and the same settings as a previous run, inference creates a similar image. The seed value is a random number.\n",
    "\n",
    "\n",
    "style_preset: the parameter includes enhance, anime, photographic, digital-art, comic-book, fantasy-art, line-art, analog-film, neon-punk, isometric, low-poly, origami, modeling-compound, cinematic, 3d-model, pixel-art, and tile-texture. This list of style presets is subject to change; refer to the latest release and documentation for updates.\n",
    "\n",
    "https://platform.stability.ai/docs/api-reference#tag/v1generation/operation/textToImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0dfdd-f35c-4ff6-9106-148da6826c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Generate a random seed value\n",
    "random_seed = random.randint(1, 9999999)  # Adjust the range as needed\n",
    "\n",
    "request = json.dumps({\n",
    "    \"text_prompts\": (\n",
    "        [{\"text\": prompt, \"weight\": 1.0}]\n",
    "        + [{\"text\": negprompt, \"weight\": -1.0} for negprompt in negative_prompts]\n",
    "    ),\n",
    "    \"cfg_scale\": 10,\n",
    "    \"seed\": random_seed,  # Assign the random seed value here\n",
    "    \"steps\": 150,\n",
    "    \"style_preset\": style_preset,\n",
    "})\n",
    "modelId = \"stability.stable-diffusion-xl\"\n",
    "\n",
    "response = bedrock_client.invoke_model(body=request, modelId=modelId)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body[\"result\"])\n",
    "base_64_img_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "print(f\"{base_64_img_str[0:80]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f0a7a-1793-4ab4-96cb-aa830fdc0bd5",
   "metadata": {},
   "source": [
    "### By decoding our Base64 string to binary, and loading it with an image processing library like Pillow that can read PNG files, we can display and manipulate the image here in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a41629b-e0ab-4518-a21d-614265a72fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "image_1 = Image.open(io.BytesIO(base64.decodebytes(bytes(base_64_img_str, \"utf-8\"))))\n",
    "image_1.save(\"data/image_1.png\")\n",
    "image_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fae8b3-a919-42c9-b769-fcd895649671",
   "metadata": {},
   "source": [
    "## Image to Image\n",
    "\n",
    "Generating images from text is powerful, but in some cases could need many rounds of prompt refinement to get an image \"just right\".\n",
    "\n",
    "Rather than starting from scratch with text each time, image-to-image generation lets us modify an existing image to make the specific changes we'd like.\n",
    "\n",
    "We'll have to pass our initial image in to the API in base64 encoding, so first let's prepare that. You can use either the initial image from the previous section, or a different one if you'd prefer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec7b5f-e248-4fed-8948-74d808f93254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_to_base64(img) -> str:\n",
    "    \"\"\"Convert a PIL Image or local image file path to a base64 string for Amazon Bedrock\"\"\"\n",
    "    if isinstance(img, str):\n",
    "        if os.path.isfile(img):\n",
    "            print(f\"Reading image from file: {img}\")\n",
    "            with open(img, \"rb\") as f:\n",
    "                return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {img} does not exist\")\n",
    "    elif isinstance(img, Image.Image):\n",
    "        print(\"Converting PIL Image to base64 string\")\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(f\"Expected str (filename) or PIL Image. Got {type(img)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ed41b-f97d-42bf-b2b1-c41c18390913",
   "metadata": {},
   "source": [
    "### Use the sample image for new image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a8183-8862-4ab9-8f69-606cf68dc45e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define image_to_image1 as a PIL Image object with your actual image file path)\n",
    "image_to_image1 = Image.open('image_to_image1.png')\n",
    "\n",
    "init_image_b64 = image_to_base64(image_to_image1)\n",
    "print(init_image_b64[:80] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad14f6c-8700-465f-8d70-88bd4f9b53e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_to_image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657202b-2dda-473d-8728-5fe8bf50d52c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request = json.dumps({\n",
    "    \"text_prompts\": (\n",
    "        [{\"text\": prompt, \"weight\": 1.0}]\n",
    "        + [{\"text\": negprompt, \"weight\": -1.0} for negprompt in negative_prompts]\n",
    "    ),\n",
    "    \"cfg_scale\": 10,\n",
    "    \"init_image\": init_image_b64,\n",
    "    \"seed\": 3661,\n",
    "    \"start_schedule\": 0.6,\n",
    "    \"steps\": 150,\n",
    "    \"style_preset\": style_preset,\n",
    "})\n",
    "modelId = \"stability.stable-diffusion-xl\"\n",
    "\n",
    "response = bedrock_client.invoke_model(body=request, modelId=modelId)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body[\"result\"])\n",
    "image_2_b64_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "print(f\"{image_2_b64_str[0:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c394b-3f5d-4016-b181-d373a870680c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_2 = Image.open(io.BytesIO(base64.decodebytes(bytes(image_2_b64_str, \"utf-8\"))))\n",
    "image_2.save(\"data/image_2.png\")\n",
    "image_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061e3b3-18bc-4276-87ea-20366303dca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
