{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3f88dd-0f5e-427e-84ee-8934982300d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Personalized Marketing Campagin Content Creation \n",
    "## Use case\n",
    "\n",
    "Assuming you are a marketing campaign manager, you're going to promote the flight ticket for XYZ Airline. At first, AI system will help you find out who are the target users; and then generate marketing promotion template for thoes target users, to protect your brand reputation and mitigate the risk of generative AI, system will use content moderation engine to inspect the content, if the compliance confidence level higher than 99%, system will save the email template into Amazon SES, marketing manager will use it sending the email to target user list. \n",
    "\n",
    "There has three AI engines\n",
    "1. Recommendation engine - Amazon Personalize service\n",
    "2. Content Generative engine - Amazon Bedrock service\n",
    "3. Content Moderation engine - Amazon Comprehend service\n",
    "\n",
    "The AI system pipelein will be - \n",
    "\n",
    "Marketing request-->Personalize-->retrive medata-->combine with PromptTemplate--> Amazon Bedrock--> Amazon Comprehend-->send to Amazon SES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b920ca4a-a71d-4630-a6e4-577d95192ad1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we show how to generate personalized marketing campaign promotion for email, by contextual metadata and question template. \n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. The key aspects of this framework allow us to augment the Large Language Models by chaining together various components to create advanced use cases.\n",
    "\n",
    "In this notebook we will use the Bedrock API provided by LangChain. The prompt used in this example creates a custom LangChain prompt template for adding context to the text generation request. \n",
    "\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. The key aspects of this framework allow us to augment the Large Models and enable us to perform tasks which meet desired goals and unlock various use-cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11828a-243d-4808-9c92-e8caf4cebd37",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "Before we get started with the implementation we have to make sure that the required boto3 and botocore packages are installed. These will be used to leverage the Amazon Bedrock API client.\n",
    "\n",
    "Additionally we would need langchain one of  the latest versions 0.0.190 which has Amazon Bedrock class implemented under llms module. Also we are installing the transformers framework from HuggingFace, which we will use to quickly count the number of tokens in the input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b7248-f29d-4abe-9aea-57471e676bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1619b-fdd7-460f-9566-0434722dd616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2c0a9-4838-4f2b-bb36-61c0cbcd62af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install dependencies/boto3-1.26.162-py3-none-any.whl\n",
    "!python3 -m pip install dependencies/botocore-1.29.162-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2be60b-480a-4524-8a1d-3529ebcb812d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain==0.0.190 --quiet\n",
    "%pip install transformers==4.24.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31093a39-704f-4a52-a4cb-d094cf930b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install ipywidgets\n",
    "\n",
    "# install timer counter \n",
    "%pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcee7d7-774f-4ade-b00e-e66052a9a8d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from langchain import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2123e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "boto3_bedrock  = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1cdad-0eda-4c06-a980-281b710f6a3a",
   "metadata": {},
   "source": [
    "## Running Amazon Personalize user segmentation job, it will generate segments of target users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc05da-f696-4a04-b681-a8cfb0ce5058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "personalize = boto3.client(service_name = 'personalize')\n",
    "s3 =boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58580ef0-ea43-4dc2-9ce2-34527a7168c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Amazon personalize user segmentation batch job, input is target promotion flight ticket, output result is target user group in gold membership\n",
    "\n",
    "import datetime\n",
    "import boto3\n",
    "\n",
    "current_datetime = datetime.datetime.now()\n",
    "formatted_datetime = current_datetime.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "job_name = f\"user-segmentation-{formatted_datetime}\"\n",
    "\n",
    "create_batch_segment_response = personalize.create_batch_segment_job(\n",
    "    jobName=job_name,\n",
    "    solutionVersionArn='arn:aws:personalize:us-east-1:<AWS account id>:solution/user-segmentation-v1/8615045e',\n",
    "    numResults=10,\n",
    "    jobInput={\n",
    "        \"s3DataSource\": {\n",
    "            \"path\": \"s3://<AWS account id>-us-east-1-personalize-user-segmentation-demo/input/user-segment-request.json\"\n",
    "        }\n",
    "    },\n",
    "    jobOutput={\n",
    "        \"s3DataDestination\": {\n",
    "            \"path\": \"s3://<AWS account id>-us-east-1-personalize-user-segmentation-demo/output/\"\n",
    "        }\n",
    "    },\n",
    "    roleArn='arn:aws:iam::<AWS account id>:role/service-role/AmazonSageMaker-ExecutionRole-20230616T101619'\n",
    ")\n",
    "\n",
    "batch_segment_job_arn = create_batch_segment_response['batchSegmentJobArn']\n",
    "print(batch_segment_job_arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7daa1a8-d21a-410c-adbf-b253c2dabf80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Content generative engine, invoke the Bedrock LLM Model\n",
    "\n",
    "for more details for the parameters please refer to the bedrock api page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f5c74-30b1-4ae8-b044-282f5232653f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "bedrock = boto3.client(\n",
    " service_name='bedrock',\n",
    " region_name='us-east-1',\n",
    " endpoint_url='https://bedrock.us-east-1.amazonaws.com'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ab211-dc91-444e-9968-3821d1e32326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock.list_foundation_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa1250-56cd-4b6d-b3d8-c62baac143ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# textgen_llm = Bedrock(model_id=\"amazon.titan-tg1-large\", client=boto3_bedrock)\n",
    "\n",
    "inference_modifier = {'max_tokens_to_sample':4096, \n",
    "                      \"temperature\":0.7,\n",
    "                      \"top_k\":250,\n",
    "                      \"top_p\":1,\n",
    "                      \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                     }\n",
    "\n",
    "textgen_llm = Bedrock(model_id = \"anthropic.claude-v1\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f310c88-bb7b-4dab-8980-ce29f5ea34fd",
   "metadata": {},
   "source": [
    "## Prepare contextual item medata, and question template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705c52c-e26d-4df7-af1d-e4ef22ec582f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_json_data(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "def on_confirm_button_click(b):\n",
    "    selected_metadata_file = metadata_dropdown.value\n",
    "    selected_template_file = template_dropdown.value\n",
    "\n",
    "    metadata = read_json_data(selected_metadata_file)\n",
    "    print(\"Metadata JSON data:\")\n",
    "    print(json.dumps(metadata, indent=4))\n",
    "\n",
    "    template = read_json_data(selected_template_file).get(\"Question\", \"\")\n",
    "    print(\"Ticketing Template:\")\n",
    "    print(template)\n",
    "\n",
    "metadata_dropdown = widgets.Dropdown(\n",
    "    options=[\"test-metadata.json\", \"other-metadata.json\"],  # Add more options if needed\n",
    "    description=\"Metadata File:\"\n",
    ")\n",
    "\n",
    "template_dropdown = widgets.Dropdown(\n",
    "    options=[\"ticketing-template.json\", \"other-template.json\"],  # Add more options if needed\n",
    "    description=\"Template File:\"\n",
    ")\n",
    "\n",
    "confirm_button = widgets.Button(description=\"Confirm button\")\n",
    "confirm_button.on_click(on_confirm_button_click)\n",
    "\n",
    "display(metadata_dropdown, template_dropdown, confirm_button)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c875074-4113-499d-9636-2d9c18b5af96",
   "metadata": {},
   "source": [
    "## Prepare the prompting template by metadata and question template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526db6f-112d-4b37-8fcd-62879c58b022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_output():\n",
    "    with open(\"test-metadata.json\", \"r\") as json_file:\n",
    "        metadata = json.load(json_file)\n",
    "\n",
    "    with open(\"ticketing-template.json\", \"r\") as json_file:\n",
    "        ticketing_template = json.load(json_file)\n",
    "\n",
    "    input_variables = list(metadata.keys())\n",
    "    template = ticketing_template.get(\"Question\", \"\")\n",
    "\n",
    "    multi_var_prompt = PromptTemplate(\n",
    "        input_variables=input_variables,\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    prompt = multi_var_prompt.format(**metadata)\n",
    "\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        print(prompt)\n",
    "    \n",
    "    \n",
    "    return prompt    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99e76d-46c4-457e-bee3-719c709a334b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "# Create the button widget\n",
    "button = widgets.Button(description=\"Generate Question\")\n",
    "\n",
    "# Define the event handler for the button\n",
    "button.on_click(update_output)\n",
    "\n",
    "# Display the button and output widget\n",
    "display(button, output)\n",
    "\n",
    "prompt = update_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460879b2-86f9-48f1-87ce-6d5348a394e0",
   "metadata": {},
   "source": [
    "## Optional, Prompt Chaining\n",
    "if you have an example template, and you want LLM to learn it, and generate new content. \n",
    "\n",
    "https://docs.anthropic.com/claude/docs/prompt-chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971af2a-c942-4b91-b70d-8c07ed1aa6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#optional, if you have example email template, and you want bedrock to refer it as a baseline example \n",
    "\n",
    "import json\n",
    "\n",
    "# Read the JSON file and extract paragraphs\n",
    "with open(\"baseline-sample.json\", \"r\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "    title = data.get(\"Email title\", \"\")\n",
    "    body = data.get(\"Email body\", \"\")\n",
    "\n",
    "\n",
    "# Additional statement\n",
    "additional_statement = \"Here is a good example for you.\"\n",
    "\n",
    "# Combine paragraphs with prompt and additional statement\n",
    "prompt = f\"{prompt}\\n\\n{additional_statement}\\n\\n{''.join(title)}\\n\\n{''.join(body)}\"\n",
    "\n",
    "# Print the combined text\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45bdfd5-ce76-42e9-81cd-b0892337d163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = textgen_llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf31e9-56c0-408f-a652-9e23de446aef",
   "metadata": {},
   "source": [
    "## Generate the content by the request of prompting question\n",
    "\n",
    "invoke using the prompt tempalate and expect to see a curated response back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f2d1e-1d11-489b-9097-f2449d01f70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you already have the 'textgen_llm' function defined\n",
    "# prompt = \"Your prompt goes here\"\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "# Measure the time taken for text generation\n",
    "start_time = time.time()\n",
    "email = response[response.index('\\n')+1:]\n",
    "end_time = time.time()\n",
    "responding_time = end_time - start_time\n",
    "\n",
    "# Print the responding time\n",
    "print(\"Responding time:\", responding_time, \"seconds\")\n",
    "\n",
    "# Show the progress bar while printing the email\n",
    "#print(\"Generating email---->\")\n",
    "#for char in tqdm(email):\n",
    "#    print(char, end='', flush=True)\n",
    "#    time.sleep(0.02)  # A small delay to make the progress bar visible\n",
    "print(\"\\nEmail generation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1064c57-27a4-48c5-911b-e4f1dfeff122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#response = textgen_llm(prompt)\n",
    "\n",
    "#email = response[response.index('\\n')+1:]  \n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628bb487-d1e9-4d28-a083-cf3b887851c5",
   "metadata": {},
   "source": [
    "## Sending the content into text content moderation engine, and check the complain, if the score is higher than 0.99, it will pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e3821-6952-4cfb-a68b-a40cc564af01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# send the email title and body for text content moderation checking\n",
    "\n",
    "import boto3\n",
    "\n",
    "region = 'us-east-1'\n",
    "comprehend = boto3.client('comprehend', region_name=region)\n",
    "\n",
    "moderation_response = comprehend.classify_document(\n",
    "    Text=email,\n",
    "    EndpointArn='arn:aws:comprehend:us-east-1:696784033931:document-classifier-endpoint/demo'\n",
    ")\n",
    "print(moderation_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e2731d-3679-4a9a-aeef-d8e9d09ae087",
   "metadata": {},
   "source": [
    "## Save the content in JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f594e-aff7-465e-b7b9-70002e2ad4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming you already have the 'email' variable with the email content\n",
    "# If not, you can replace this with the appropriate content.\n",
    "\n",
    "email_content = email.strip()\n",
    "\n",
    "# Create a dictionary with the email content\n",
    "email_dict = {\"email\": email_content}\n",
    "\n",
    "# Define the filename for the JSON file\n",
    "json_filename = \"email_content_english.json\"\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open(json_filename, \"w\") as json_file:\n",
    "    json.dump(email_dict, json_file, indent=4)\n",
    "\n",
    "print(f\"The email content has been saved as {json_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d60361-1425-41be-8dff-12a515a6a7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Step 1: Read the content from the original JSON file\n",
    "with open('email_content_english.json', 'r', encoding='utf-8') as file:\n",
    "    data_str = file.read()\n",
    "    print(\"Data read from the file:\")\n",
    "    print(data_str)\n",
    "\n",
    "# Step 2: Extract the email content as string from the nested JSON-like structure\n",
    "try:\n",
    "    data = json.loads(data_str)\n",
    "    email_content_str = data.get('email')\n",
    "    print(\"Email content as string:\")\n",
    "    print(email_content_str)\n",
    "\n",
    "    # Step 3: Clean the email content string from invalid control characters\n",
    "    email_content_str_cleaned = re.sub(r'[\\t\\n\\r]', '', email_content_str)\n",
    "\n",
    "    # Step 4: Extract the actual JSON data from the cleaned email_content_str\n",
    "    email_data_start = email_content_str_cleaned.find(\"{\")\n",
    "    email_data_end = email_content_str_cleaned.rfind(\"}\") + 1\n",
    "    email_data_str = email_content_str_cleaned[email_data_start:email_data_end]\n",
    "    email_content = json.loads(email_data_str)\n",
    "\n",
    "    # Step 5: Check if the required fields ('Email title' and 'Email body') exist in the loaded JSON data\n",
    "    if isinstance(email_content, dict) and \"Email title\" in email_content and \"Email body\" in email_content:\n",
    "        # Step 6: Extract the required fields ('Email title' and 'Email body')\n",
    "        email_title = email_content[\"Email title\"]\n",
    "        email_body = email_content[\"Email body\"]\n",
    "\n",
    "        # Step 7: Create a new dictionary with the extracted data\n",
    "        extracted_data = {\n",
    "            \"Email title\": email_title,\n",
    "            \"Email body\": email_body\n",
    "        }\n",
    "\n",
    "        # Step 8: Write the new JSON object into a new file\n",
    "        with open('new_email_content.json', 'w', encoding='utf-8') as file:\n",
    "            json.dump(extracted_data, file, indent=4)\n",
    "\n",
    "        print(\"Extraction and saving completed.\")\n",
    "    else:\n",
    "        print(\"The required fields ('Email title' and 'Email body') are missing in the JSON data.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"Error decoding JSON data:\", e)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb882804-991d-455e-9a5a-86148dd4992d",
   "metadata": {},
   "source": [
    "## Save the content into Amazon Simple Email service template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091ae57-e528-4cca-921c-5d8a053a0a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#You can create up to 20,000 email templates in each AWS Region.\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure AWS credentials (make sure you have appropriate IAM permissions)\n",
    "aws_access_key_id = \"<your id>\"\n",
    "aws_secret_access_key = \"<your key>\"\n",
    "aws_region = \"us-east-1\"\n",
    "\n",
    "# Load the email template from the JSON file\n",
    "with open(\"new_email_content.json\", \"r\") as json_file:\n",
    "    email_template = json.load(json_file)\n",
    "\n",
    "# Generate dynamic TemplateName\n",
    "current_datetime = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "template_name = f\"campaign{current_datetime}\"\n",
    "\n",
    "# Create a new SES client\n",
    "ses_client = boto3.client(\n",
    "    \"ses\",\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=aws_region,\n",
    ")\n",
    "\n",
    "# Create the email template\n",
    "response = ses_client.create_template(\n",
    "    Template={\n",
    "        \"TemplateName\": template_name,\n",
    "        \"SubjectPart\": email_template[\"Email title\"],\n",
    "        \"HtmlPart\": email_template[\"Email body\"],\n",
    "        \"TextPart\": \"For demo email\",  # You can provide a text version here if needed\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8671b-2715-4272-b6ac-4dd7eac19534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d2369-d6ab-453a-80be-0116028c649c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the email template\n",
    "response = ses_client.get_template(TemplateName=template_name)\n",
    "\n",
    "# Extract the details of the email template\n",
    "template_subject = response['Template']['SubjectPart']\n",
    "template_html_body = response['Template']['HtmlPart']\n",
    "template_text_body = response['Template']['TextPart']\n",
    "\n",
    "# Display the template details\n",
    "print(f\"Template Name: {template_name}\")\n",
    "print(f\"Template Subject: {template_subject}\")\n",
    "print(f\"Template HTML Body:\\n{template_html_body}\")\n",
    "print(f\"Template Text Body:\\n{template_text_body}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbbbca-bea3-436f-8066-8139d245f2a3",
   "metadata": {},
   "source": [
    "## Marketing team can use the template to send email to target users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b6567-d887-46f6-ac70-d821dec91dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the email address to send the email to\n",
    "recipient_email = \"target_user_email\"  # Replace with the recipient's email address\n",
    "\n",
    "# Define replacement data for the template placeholders\n",
    "replacement_data = {\n",
    "    \"Customer name\": \"name\",\n",
    "    \"phone number\": \"number\",\n",
    "    \"email address\": \"email\",\n",
    "    \"date\": \"20-08-2023\"\n",
    "}\n",
    "\n",
    "# Send the email using the email template\n",
    "response = ses_client.send_templated_email(\n",
    "    Source=\"sender email address\",  # Replace with the verified sender email address in Amazon SES\n",
    "    Destination={\n",
    "        \"ToAddresses\": [recipient_email],\n",
    "    },\n",
    "    Template=template_name,\n",
    "    TemplateData=json.dumps(replacement_data),\n",
    ")\n",
    "\n",
    "print(\"Email sent successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5eaa1-c21f-4146-9cbe-d9cb9f8e4222",
   "metadata": {},
   "source": [
    "# Asia language support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8aa4f-40b2-4008-bd21-1a8ce699ed30",
   "metadata": {},
   "source": [
    "## Bahasa Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f4c69-e7ba-481f-8e5d-a056feff97a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Bahasa Indonesia\n",
    "# Create a prompt template that has multiple input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"DSTCity\", \"Season\", \"Airline\", \"memberClass\"], \n",
    "    template=\"\"\"I will promote flight ticket of {DSTCity} during {Season} to my customer, \n",
    "    I want to generate an attractive e-mail title and body to promote the flight ticket of Airline {Airline} to the city of {DSTCity} during {Season} for {memberClass} membership, \n",
    "    pls help to write a body of words with landscape itinerary details, with an attractive title to help me to promote the flight ticket to the customers. please generate the content for the promotion email in Bahasa Indonesia with an attractive title and 400 words body based on the metadata and prompt template.\n",
    "   \"\"\"\n",
    "    \n",
    ")\n",
    "\n",
    "# Pass in values to the input variables\n",
    "prompt = multi_var_prompt.format(DSTCity=\"Tokyo\", \n",
    "                                 Season=\"April and May\", \n",
    "                                 Airline=\"Singapore Airline\",\n",
    "                                 memberClass=\"Gold\"\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846440e-d09c-45e0-a127-db549206d5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = textgen_llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33c050-4fdb-40e1-a169-299aeb863573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = textgen_llm(prompt)\n",
    "\n",
    "email = response[response.index('\\n')+1:]  \n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcaea9-5bc2-4fec-9b1f-2a4a158b0722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming you already have the 'email' variable with the email content\n",
    "# If not, you can replace this with the appropriate content.\n",
    "\n",
    "email_content = email.strip()\n",
    "\n",
    "# Create a dictionary with the email content\n",
    "email_dict = {\"email\": email_content}\n",
    "\n",
    "# Define the filename for the JSON file\n",
    "json_filename = \"email_content_Bahasa.json\"\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open(json_filename, \"w\") as json_file:\n",
    "    json.dump(email_dict, json_file, indent=4)\n",
    "\n",
    "print(f\"The email content has been saved as {json_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fcf2b6-685f-4221-a157-843e664a274b",
   "metadata": {},
   "source": [
    "## Thai language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849ab1d-790f-4e82-88f4-2bcf9af7bb77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Thai language\n",
    "# Create a prompt template that has multiple input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"DSTCity\", \"Season\", \"Airline\", \"memberClass\"], \n",
    "    template=\"\"\"I will promote flight ticket of {DSTCity} during {Season} to my customer, \n",
    "    I want to generate an attractive e-mail title and body to promote the flight ticket of Airline {Airline} to the city of {DSTCity} during {Season} for {memberClass} membership, \n",
    "    pls help to write a body of words with landscape itinerary details, with an attractive title to help me to promote the flight ticket to the customers. please generate the content for the promotion email in Thai language with an attractive title and 400 words body based on the metadata and prompt template.\n",
    "   \"\"\"\n",
    "    \n",
    ")\n",
    "\n",
    "# Pass in values to the input variables\n",
    "prompt = multi_var_prompt.format(DSTCity=\"Tokyo\", \n",
    "                                 Season=\"April and May\", \n",
    "                                 Airline=\"Singapore Airline\",\n",
    "                                 memberClass=\"Gold\"\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b5b03-5cff-4eed-b9e1-f5c9d52e8150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = textgen_llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbe53d-a8d2-456f-806b-b6ca1bf429ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = textgen_llm(prompt)\n",
    "\n",
    "email = response[response.index('\\n')+1:]  \n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cea1f9-76e4-41b8-ba30-6312f5fdd9ed",
   "metadata": {},
   "source": [
    "## Vietnamese language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f564bd8-b852-492b-8ad2-6b496f1cfb94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Vietnamese language\n",
    "# Create a prompt template that has multiple input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"DSTCity\", \"Season\", \"Airline\", \"memberClass\"], \n",
    "    template=\"\"\"I will promote flight ticket of {DSTCity} during {Season} to my customer, \n",
    "    I want to generate an attractive e-mail title and body to promote the flight ticket of Airline {Airline} to the city of {DSTCity} during {Season} for {memberClass} membership, \n",
    "    pls help to write a body of words with landscape itinerary details, with an attractive title to help me to promote the flight ticket to the customers. please generate the content for the promotion email in Vietnamese language with an attractive title and 400 words body based on the metadata and prompt template.\n",
    "   \"\"\"\n",
    "    \n",
    ")\n",
    "\n",
    "# Pass in values to the input variables\n",
    "prompt = multi_var_prompt.format(DSTCity=\"Tokyo\", \n",
    "                                 Season=\"April and May\", \n",
    "                                 Airline=\"Singapore Airline\",\n",
    "                                 memberClass=\"Gold\"\n",
    "     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e9d6c4-568b-4f77-a0f7-119ab2f30ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = textgen_llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10d7d2-027f-49b9-b31d-f18f47ea4126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = textgen_llm(prompt)\n",
    "\n",
    "email = response[response.index('\\n')+1:]  \n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590668df-4e7b-4a56-a579-d04b4166db05",
   "metadata": {},
   "source": [
    "## Chinese language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e50f71-ffdd-424e-9e96-8f29997486f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Vietnamese language\n",
    "# Create a prompt template that has multiple input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"DSTCity\", \"Season\", \"Airline\", \"memberClass\"], \n",
    "    template=\"\"\"I will promote flight ticket of {DSTCity} during {Season} to my customer, \n",
    "    I want to generate an attractive e-mail title and body to promote the flight ticket of Airline {Airline} to the city of {DSTCity} during {Season} for {memberClass} membership, \n",
    "    pls help to write a body of words with landscape itinerary details, with an attractive title to help me to promote the flight ticket to the customers. please generate the content for the promotion email in chinese language with an attractive title and 400 words body based on the metadata and prompt template.\n",
    "   \"\"\"\n",
    "    \n",
    ")\n",
    "\n",
    "# Pass in values to the input variables\n",
    "prompt = multi_var_prompt.format(DSTCity=\"Tokyo\", \n",
    "                                 Season=\"April and May\", \n",
    "                                 Airline=\"Singapore Airline\",\n",
    "                                 memberClass=\"Gold\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c4957-ffe7-46b1-aa68-76a852e82eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_tokens = textgen_llm.get_num_tokens(prompt)\n",
    "print(f\"Our prompt has {num_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f7b00-5dc7-4978-b8f4-158923025524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = textgen_llm(prompt)\n",
    "\n",
    "email = response[response.index('\\n')+1:]  \n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9abc40",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "To conclude we learnt that invoking the LLM without any context might not yeild the desired results. By adding contextual data and further using the the prompt template to constrain the output from the LLM we are able to successfully get our desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82520d5-68e5-4a5d-b4f8-5939a566287b",
   "metadata": {},
   "source": [
    "## Text to Image (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536c72e-8dfa-4872-bcde-506ab7a9483f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --quiet \"pillow>=9.5,<10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a988d2-20b9-48ba-a1b0-ea8dfcfd528c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4dfcfb-fe71-481e-94f7-881c1b3739b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from PIL import Image\n",
    "bedrock_client = boto3.client('bedrock' , 'us-east-1', endpoint_url='https://bedrock.us-east-1.amazonaws.com')\n",
    "bedrock_client.list_foundation_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab780d8-19aa-450c-ae0d-c83a33a3c544",
   "metadata": {},
   "source": [
    "## Prepare prompting\n",
    "The promopting can be generated by last step of text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a59ed5-4fbf-452b-b4f0-8a581c80b5eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"I will promote flight ticket of Airline ticket, from Singapore to Tokyo, during ['April', 'May'] for Gold membership, pls create a cover photographic, without human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dca501-3c1d-4540-b953-3df7a8620bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "negative_prompts = [\n",
    "    \"poorly rendered\",\n",
    "    \"poor background details\",\n",
    "    \"Not realistic enough\",\n",
    "    \"poorly drawn people\",\n",
    "    \"poorly drawn human eyes\",\n",
    "    \"poorly drawn human nose\",\n",
    "    \"poorly drawn human hair\",\n",
    "    \"poorly drawn human finger\",\n",
    "    \"missing human fingers\",\n",
    "    \"poorly drawn aircraft\",\n",
    "    \"poorly quality of font\",\n",
    "    \"pixels too lower\",\n",
    "    \"disfigured people features\",\n",
    "]\n",
    "style_preset = \"photographic\"  # (e.g. photographic, digital-art, cinematic, ...)\n",
    "#prompt = \"photo taken from above of an italian landscape. cloud is clear with few clouds. Green hills and few villages, a lake\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253cdf8d-c7bb-4808-947b-95473ab5a4eb",
   "metadata": {},
   "source": [
    "### The Stability.ai Diffusion models support the following controls. \n",
    "\n",
    "cfg_scale: Prompt strength– Determines how much the final image portrays the prompt random generations. The range is 0—30, and the default value is 10.\n",
    "the \"cfg_scale\" essentially governs how much the image looks closer to the prompt or input image. The higher the CFG scale, the more the image will match your prompt. Conversely, a lower CFG scale value produces a better-quality image that may differ from the original prompt or image\n",
    "\n",
    "In Stable Diffusion, CFG stands for Classifier Free Guidance scale. CFG is the setting that controls how closely Stable Diffusion should follow your text prompt. It is applied in text-to-image (txt2img) and image-to-image (img2img) generations.\n",
    "\n",
    "The higher the CFG value, the more strictly it will follow your prompt, in theory. The default value is 10, which gives a good balance between creative freedom and following your direction. A value of 1 will give Stable Diffusion almost complete freedom, whereas values above 15 are quite restrictive.\n",
    "\n",
    "step: Generation step determines how many times the image is sampled. More steps can result in a more accurate result. The range is 0—150, and the default value is 5.\n",
    "\n",
    "Seed: The seed determines the initial noise setting. If you use the same seed and the same settings as a previous run, inference creates a similar image. The seed value is a random number.\n",
    "\n",
    "\n",
    "style_preset: the parameter includes enhance, anime, photographic, digital-art, comic-book, fantasy-art, line-art, analog-film, neon-punk, isometric, low-poly, origami, modeling-compound, cinematic, 3d-model, pixel-art, and tile-texture. This list of style presets is subject to change; refer to the latest release and documentation for updates.\n",
    "\n",
    "https://platform.stability.ai/docs/api-reference#tag/v1generation/operation/textToImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eaf870-af0f-42d6-8728-1d557b7e5162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request = json.dumps({\n",
    "    \"text_prompts\": (\n",
    "        [{\"text\": prompt, \"weight\": 1.0}]\n",
    "        + [{\"text\": negprompt, \"weight\": -1.0} for negprompt in negative_prompts]\n",
    "    ),\n",
    "    \"cfg_scale\": 10,\n",
    "    \"seed\": 11789,\n",
    "    \"steps\": 150,\n",
    "    \"style_preset\": style_preset,\n",
    "})\n",
    "modelId = \"stability.stable-diffusion-xl\"\n",
    "\n",
    "response = bedrock_client.invoke_model(body=request, modelId=modelId)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body[\"result\"])\n",
    "base_64_img_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "print(f\"{base_64_img_str[0:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f0a7a-1793-4ab4-96cb-aa830fdc0bd5",
   "metadata": {},
   "source": [
    "### By decoding our Base64 string to binary, and loading it with an image processing library like Pillow that can read PNG files, we can display and manipulate the image here in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a41629b-e0ab-4518-a21d-614265a72fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "image_1 = Image.open(io.BytesIO(base64.decodebytes(bytes(base_64_img_str, \"utf-8\"))))\n",
    "image_1.save(\"data/image_1.png\")\n",
    "image_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fae8b3-a919-42c9-b769-fcd895649671",
   "metadata": {},
   "source": [
    "## Image to Image\n",
    "\n",
    "Generating images from text is powerful, but in some cases could need many rounds of prompt refinement to get an image \"just right\".\n",
    "\n",
    "Rather than starting from scratch with text each time, image-to-image generation lets us modify an existing image to make the specific changes we'd like.\n",
    "\n",
    "We'll have to pass our initial image in to the API in base64 encoding, so first let's prepare that. You can use either the initial image from the previous section, or a different one if you'd prefer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec7b5f-e248-4fed-8948-74d808f93254",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_to_base64(img) -> str:\n",
    "    \"\"\"Convert a PIL Image or local image file path to a base64 string for Amazon Bedrock\"\"\"\n",
    "    if isinstance(img, str):\n",
    "        if os.path.isfile(img):\n",
    "            print(f\"Reading image from file: {img}\")\n",
    "            with open(img, \"rb\") as f:\n",
    "                return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {img} does not exist\")\n",
    "    elif isinstance(img, Image.Image):\n",
    "        print(\"Converting PIL Image to base64 string\")\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    else:\n",
    "        raise ValueError(f\"Expected str (filename) or PIL Image. Got {type(img)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a8183-8862-4ab9-8f69-606cf68dc45e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define image_to_image1 as a PIL Image object with your actual image file path)\n",
    "image_to_image1 = Image.open('image_to_image1.png')\n",
    "\n",
    "init_image_b64 = image_to_base64(image_to_image1)\n",
    "print(init_image_b64[:80] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad14f6c-8700-465f-8d70-88bd4f9b53e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#os.makedirs(\"data\", exist_ok=True)\n",
    "#image_to_image1 = Image.open(io.BytesIO(base64.decodebytes(bytes(init_image_b64, \"utf-8\"))))\n",
    "#image_to_image1.save(\"data/image_to_image1.png\")\n",
    "image_to_image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657202b-2dda-473d-8728-5fe8bf50d52c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "request = json.dumps({\n",
    "    \"text_prompts\": (\n",
    "        [{\"text\": prompt, \"weight\": 1.0}]\n",
    "        + [{\"text\": negprompt, \"weight\": -1.0} for negprompt in negative_prompts]\n",
    "    ),\n",
    "    \"cfg_scale\": 10,\n",
    "    \"init_image\": init_image_b64,\n",
    "    \"seed\": 3661,\n",
    "    \"start_schedule\": 0.6,\n",
    "    \"steps\": 150,\n",
    "    \"style_preset\": style_preset,\n",
    "})\n",
    "modelId = \"stability.stable-diffusion-xl\"\n",
    "\n",
    "response = bedrock_client.invoke_model(body=request, modelId=modelId)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(response_body[\"result\"])\n",
    "image_2_b64_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "print(f\"{image_2_b64_str[0:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c394b-3f5d-4016-b181-d373a870680c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_2 = Image.open(io.BytesIO(base64.decodebytes(bytes(image_2_b64_str, \"utf-8\"))))\n",
    "image_2.save(\"data/image_2.png\")\n",
    "image_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061e3b3-18bc-4276-87ea-20366303dca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
